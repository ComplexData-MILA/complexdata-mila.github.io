---
title: Are Large Language Models Good Temporal Graph Learners?
venue: ''
openAccessPdf:
  url: ''
  status:
  license:
  disclaimer: 'Notice: This content is from the open access paper or abstract available
    at https://arxiv.org/abs/2506.05393, which is subject to the license by the author
    or copyright owner provided with this content. Please go to the source to verify
    the license and copyright information for your use.'
names: Shenyang Huang, Alipanah Parviz, Emma Kondrup, Zachary Yang, Zifeng Ding, Michael
  Bronstein, Reihaneh Rabbany, Guillaume Rabusseau
tags:
- ''
link: https://arxiv.org/abs/2506.05393
author: Shenyang Huang
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Large Language Models (LLMs) have recently driven significant advancements in Natural Language Processing and various other applications. While a broad range of literature has explored the graph-reasoning capabilities of LLMs, including their use of predictors on graphs, the application of LLMs to dynamic graphs -- real world evolving networks -- remains relatively unexplored. Recent work studies synthetic temporal graphs generated by random graph models, but applying LLMs to real-world temporal graphs remains an open question. To address this gap, we introduce Temporal Graph Talker (TGTalker), a novel temporal graph learning framework designed for LLMs. TGTalker utilizes the recency bias in temporal graphs to extract relevant structural information, converted to natural language for LLMs, while leveraging temporal neighbors as additional information for prediction. TGTalker demonstrates competitive link prediction capabilities compared to existing Temporal Graph Neural Network (TGNN) models. Across five real-world networks, TGTalker performs competitively with state-of-the-art temporal graph methods while consistently outperforming popular models such as TGN and HTGN. Furthermore, TGTalker generates textual explanations for each prediction, thus opening up exciting new directions in explainability and interpretability for temporal link prediction. The code is publicly available at https://github.com/shenyangHuang/TGTalker.