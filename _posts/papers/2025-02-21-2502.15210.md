---
title: 'PairBench: Are Vision-Language Models Reliable at Comparing What They See?'
venue: ''
openAccessPdf:
  url: ''
  status:
  license:
  disclaimer: 'Notice: This content is from the open access paper or abstract available
    at https://arxiv.org/abs/2502.15210, which is subject to the license by the author
    or copyright owner provided with this content. Please go to the source to verify
    the license and copyright information for your use.'
names: Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Valentina
  Zantedeschi, Spandana Gella, Joao Monteiro
tags:
- ''
link: https://arxiv.org/abs/2502.15210
author: Aarash Feizi
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Understanding how effectively large vision language models (VLMs) compare visual inputs is crucial across numerous applications, yet this fundamental capability remains insufficiently assessed. While VLMs are increasingly deployed for tasks requiring comparative judgment, including automated evaluation, re-ranking, and retrieval-augmented generation, no systematic framework exists to measure their performance in these scenarios. We present PairBench, a simple framework that evaluates VLMs as customizable similarity tools using widely available image datasets. Our approach introduces four key metrics for reliable comparison: alignment with human annotations, consistency across pair ordering, distribution smoothness, and controllability through prompting. Our analysis reveals that no model consistently excels across all metrics, with each demonstrating distinct strengths and weaknesses. Most concerning is the widespread inability of VLMs to maintain symmetric similarity scores. Interestingly, we demonstrate that performance on our benchmark strongly correlates with popular benchmarks used for more complex tasks, while providing additional metrics into controllability, smoothness and ordering. This makes PairBench a unique and comprehensive framework to evaluate the performance of VLMs for automatic evaluation depending on the task.