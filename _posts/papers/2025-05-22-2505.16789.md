---
title: 'Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability'
venue: ''
openAccessPdf:
  url: ''
  status:
  license:
  disclaimer: 'Notice: This content is from the open access paper or abstract available
    at https://arxiv.org/abs/2505.16789, which is subject to the license by the author
    or copyright owner provided with this content. Please go to the source to verify
    the license and copyright information for your use.'
names: Punya Syon Pandey, Samuel Simko, Kellin Pelrine, Zhijing Jin
tags:
- ''
link: https://arxiv.org/abs/2505.16789
author: Kellin Pelrine
categories: Publications

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

As large language models gain popularity, their vulnerability to adversarial attacks remains a primary concern. While fine-tuning models on domain-specific datasets is often employed to improve model performance, it can introduce vulnerabilities within the underlying model. In this work, we investigate Accidental Misalignment, unexpected vulnerabilities arising from characteristics of fine-tuning data. We begin by identifying potential correlation factors such as linguistic features, semantic similarity, and toxicity within our experimental datasets. We then evaluate the adversarial performance of these fine-tuned models and assess how dataset factors correlate with attack success rates. Lastly, we explore potential causal links, offering new insights into adversarial defense strategies and highlighting the crucial role of dataset design in preserving model alignment. Our code is available at https://github.com/psyonp/accidental_misalignment.